# 语言模型分类体系详解


一、语言模型的大类归属

因果语言模型属于 自回归语言模型（Autoregressive Language Model）大类，
是 预训练语言模型（Pretrained Language Model）的一种架构类型。

预训练语言模型 (Pretrained LM)
       │
       ├─── 按架构分类 ───────────────────────────────┐
       │                                              │
       │   ┌─────────────┬─────────────┬───────────┐ │
       │   ▼             ▼             ▼           │ │
       │  因果 LM       掩码 LM       前缀 LM      │ │
       │  (Causal)     (Masked)      (Prefix)     │ │
       │   │             │             │           │ │
       │   ▼             ▼             ▼           │ │
       │  解码器       编码器        编码器 -      │ │
       │  架构         架构          解码器架构    │ │
       │                                              │
       └──────────────────────────────────────────────┘

================================================================================

二、三大语言模型类型详细对比

| 维度         | 因果语言模型        | 掩码语言模型        | 前缀语言模型            |
|--------------|---------------------|---------------------|-------------------------|
| 英文         | Causal LM           | Masked LM           | Prefix LM               |
| 架构类型     | 解码器 (Decoder)    | 编码器 (Encoder)    | 编码器 - 解码器           |
| 注意力方向   | 单向 (←)            | 双向 (↔)            | 前缀双向 + 后缀单向        |
| 训练目标     | 预测下一个词        | 预测被掩码的词      | 预测目标序列            |
| 典型用途     | 文本生成、对话      | 文本理解、分类      | 翻译、摘要              |

================================================================================

三、各类型代表模型列表

【1】因果语言模型（Causal LM / Decoder-only）

| 模型系列      | 开发者        | 年份      | 特点                    |
|---------------|---------------|-----------|-------------------------|
| GPT           | OpenAI        | 2018-2023 | 开创者，GPT-4最强       |
| GPT-2         | OpenAI        | 2019      | 开源，1.5B参数          |
| GPT-3         | OpenAI        | 2020      | 175B参数，few-shot      |
| Llama         | Meta          | 2023      | 开源代表，7B-65B        |
| Llama 2       | Meta          | 2023      | 商业友好，对话优化      |
| Llama 3       | Meta          | 2024      | 8B-70B，性能提升        |
| Qwen/通义千问 | 阿里巴巴      | 2023-2025 | 中文优化，多模态        |
| Qwen 2/2.5    | 阿里巴巴      | 2024      | 开源，多语言            |
| Falcon        | TII           | 2023      | 高效架构，40B-180B      |
| Baichuan/百川 | 百川智能      | 2023-2024 | 中文友好，开源          |
| ChatGLM       | 智谱AI        | 2023-2024 | 中英双语，高效          |
| Mistral       | Mistral AI    | 2023-2024 | 高效小模型，7B          |
| Mixtral       | Mistral AI    | 2024      | MoE架构，8x7B           |
| DialoGPT      | Microsoft     | 2020      | 对话专用              |
| CodeGen       | Salesforce    | 2022      | 代码生成专用          |
| StarCoder     | BigCode       | 2023      | 代码生成，开源        |
| Phi-2/3       | Microsoft     | 2023-2024 | 小模型，高质量数据    |

因果语言模型架构示意：

输入：[我] [爱] [学] [习] [?]
       │   │   │   │   │
       ▼   ▼   ▼   ▼   ▼
    ┌─────────────────────┐
    │   Decoder Layers    │  ← 只有解码器层
    │   (因果注意力掩码)   │
    └─────────────────────┘
       │   │   │   │   │
       ▼   ▼   ▼   ▼   ▼
输出：[爱] [学] [习] [机] [器]

特点：从左到右单向生成，适合连续文本生成

--------------------------------------------------------------------------------

【2】掩码语言模型（Masked LM / Encoder-only）

| 模型系列          | 开发者          | 年份   | 特点                    |
|-------------------|-----------------|--------|-------------------------|
| BERT              | Google          | 2018   | 开创者，双向注意力      |
| RoBERTa           | Meta            | 2019   | BERT优化版，更大数据    |
| ALBERT            | Google          | 2019   | 参数共享，轻量级        |
| DistilBERT        | Hugging Face    | 2019   | 蒸馏版，60%参数         |
| DeBERTa           | Microsoft       | 2020   | 解耦注意力，更强        |
| Electra           | Google          | 2020   | 高效训练，判别式        |
| XLM-RoBERTa       | Meta            | 2020   | 多语言，100+语言        |
| Chinese-BERT      | 哈工大          | 2019   | 中文优化                |
| MacBERT           | 哈工大          | 2020   | 中文纠错优化            |
| WoBERT            | 百度            | 2020   | 中文词级BERT            |
| LayoutLM          | Microsoft       | 2020   | 文档理解，多模态        |
| CodeBERT          | Microsoft       | 2020   | 代码理解专用            |

掩码语言模型架构示意：

输入：[我] [爱] [MASK] [习] [机]
       │   │    │    │   │
       └───┴────┴────┴───┘
            ↕ 双向注意力 ↕
       ┌─────────────────────┐
       │   Encoder Layers    │  ← 只有编码器层
       │   (双向注意力)      │
       └─────────────────────┘
            │    │    │    │
            ▼    ▼    ▼    ▼
输出：[我] [爱] [学] [习] [机]
            ↑
       预测被掩码的词

特点：双向上下文理解，适合分类、NER等理解任务

--------------------------------------------------------------------------------

【3】前缀语言模型（Prefix LM / Encoder-Decoder）

| 模型系列      | 开发者        | 年份   | 特点                    |
|---------------|---------------|--------|-------------------------|
| T5            | Google        | 2020   | 统一文本到文本框架      |
| T5-Large/XL   | Google        | 2020   | 不同规模版本            |
| mT5           | Google        | 2021   | 多语言T5，101语言       |
| BART          | Meta          | 2020   | 去噪自编码器            |
| BART-Large    | Meta          | 2020   | 更大规模版本            |
| mBART         | Meta          | 2020   | 多语言BART              |
| FLAN-T5       | Google        | 2022   | 指令微调版T5            |
| GLM           | 智谱AI        | 2022   | 通用语言模型            |
| GLM-130B      | 智谱AI        | 2022   | 130B参数                |
| ChatGLM       | 智谱AI        | 2023   | 对话优化版              |
| PEGASUS       | Google        | 2020   | 摘要专用                |
| MarianMT      | OPUS          | 2020   | 翻译专用                |
| NLLB          | Meta          | 2022   | 多语言翻译              |
| M2M100        | Meta          | 2021   | 100语言翻译             |

前缀语言模型架构示意：

输入(编码): [将] [下] [列] [英] [文] [翻] [译] [为] [中] [文]
       │   │   │   │   │   │   │   │   │   │
       └───┴───┴───┴───┴───┴───┴───┴───┴───┘
                   ↕ 双向注意力 ↕
            ┌─────────────────┐
            │   Encoder       │  ← 编码器(双向)
            └────────┬────────┘
                     │
                     ▼
            ┌─────────────────┐
输出(解码): │   Decoder       │  ← 解码器(单向)
            └────────┬────────┘
                     │
                     ▼
       [人] [工] [智] [能] [很] [强] [大]

特点：编码 - 解码分离，适合翻译、摘要等序列转换任务

================================================================================

四、完整分类对比表

| 特性         | 因果 LM              | 掩码 LM              | 前缀 LM              |
|--------------|----------------------|----------------------|----------------------|
| 架构         | Decoder-only         | Encoder-only         | Encoder-Decoder      |
| 注意力       | 单向掩码             | 双向                 | 编码双向 + 解码单向    |
| 训练目标     | 下一词预测           | 掩码词预测           | 序列到序列           |
| 推理方式     | 自回归生成           | 不生成/分类          | 自回归生成           |
| 适合任务     | 生成、对话、续写     | 分类、NER、理解      | 翻译、摘要、改写     |
| 典型模型     | GPT、Llama、Qwen     | BERT、RoBERTa        | T5、BART、GLM        |
| HF类         | AutoModelForCausalLM | AutoModelForMaskedLM | AutoModelForSeq2SeqLM|

================================================================================

五、模型类型选择指南

你的任务是什么？
       │
    ┌──┴────┬────────────┬─────────────┐
    ▼       ▼            ▼             ▼
  文本生成  文本理解    序列转换      对话系统
    │       │            │             │
    ▼       ▼            ▼             ▼
  因果 LM  掩码 LM     前缀 LM      因果 LM
  (GPT)   (BERT)      (T5)        (Llama)

具体场景:
• 写文章/代码    → 因果 LM
• 情感分析       → 掩码 LM
• 机器翻译       → 前缀 LM
• 智能客服       → 因果 LM
• 命名实体识别   → 掩码 LM
• 文本摘要       → 前缀 LM

================================================================================

六、总结

| 问题                           | 答案                                      |
|--------------------------------|-------------------------------------------|
| 因果语言模型属于什么大类       | 自回归语言模型 / 解码器架构预训练模型     |
| 因果 LM 代表模型               | GPT系列、Llama系列、Qwen、Falcon、Baichuan等 |
| 掩码 LM 代表模型               | BERT、RoBERTa、DeBERTa、ALBERT等          |
| 前缀 LM 代表模型               | T5、BART、GLM、mT5、FLAN-T5等             |
| SFT/指令微调用哪种             | 主要用因果 LM（生成任务需要）             |

核心记忆点:
• 因果 LM = 生成 = GPT/Llama/Qwen = Decoder-only
• 掩码 LM = 理解 = BERT/RoBERTa = Encoder-only
• 前缀 LM = 转换 = T5/BART = Encoder-Decoder

================================================================================